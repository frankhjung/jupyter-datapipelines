# Kaggle Training - Data Pipelines

The following are summary notes (see Jupyter notebook for more complete notes),
on [Kaggle](https://www.kaggle.com/)'s, [Getting Started with Automated Data
Pipelines](https://www.kaggle.com/professional-skills-series#pipelines?utm_medium=email&utm_source=intercom&utm_campaign=pipelines-event),
professional skills series by Rachael Tatman.

The series is specific to Kaggle. I have externalised this so the scripts can
also run on GitLab.

## Day 1 - Versioning Data

  * introduction to dataset versioning tools
  * exercise to adjust versioning a dataset on [Kaggle](https://www.kaggle.com/)

## Day 2 - Validation & Creating Datasets from URLs

  * loading data from a URL using Python3
  * loading data from a URL using R3.5.2

## Day 3 - ETL &amp; Creating Datasets from Kernel Output

  * basic principles of Extract, Transform & Load (aka ETL) pipelines
  * creating datasets from Kaggle Kernel outputs

## Dependencies

  * Use either of these two Docker images
    * [clutteredcode/python3-alpine-pandas](https://hub.docker.com/r/clutteredcode/python3-alpine-pandas)
    * [python:3.7.2-stretch](https://hub.docker.com/_/python)
  * [Python 3](https://python.org)
  * [pandas](https://pandas.pydata.org)
  * [numpy](https://www.numpy.org)
  * [GNU R](https://www.r-project.org/)

