{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwcGjG84KJ6C"
   },
   "source": [
    "# Kaggle Data Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The following are summary notes (see Jupyter notebook for more complete notes), on [Kaggle](https://www.kaggle.com/)'s, [Getting Started with Automated Data Pipelines](https://www.kaggle.com/professional-skills-series#pipelines?utm_medium=email&utm_source=intercom&utm_campaign=pipelines-event), professional skills series by Rachael Tatman.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlXUwJizM-eH"
   },
   "source": [
    "## Day 1 - Dataset Versioning\n",
    "\n",
    "Open Source Tools for versioning datasets:\n",
    "\n",
    "* [Data Version Control(DVC)](https://dvc.org/) - This is a commandline only tool based on [Git](https://git-scm.com/)\n",
    "* [Git Large File Storage](https://git-lfs.github.com/) - replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server like [GitHub.com](https://github.com/).\n",
    "\n",
    "### Versioning a Dataset on Kaggle\n",
    "\n",
    "Example: https://www.kaggle.com/frankhjung/debian-data/settings\n",
    "\n",
    "![Set versioning](https://raw.githubusercontent.com/frankhjung/jupyter-datapipelines/master/images/dataset-versioning.png)\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "* [A Practical Taxonomy of Reproducibility for Machine Learning Research (PDF)](https://openreview.net/pdf?id=B1eYYK5QgX)\n",
    "* [Dashboarding with Notebooks](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmEXijjTNN8O"
   },
   "source": [
    "## Day 2 - Validation and URLs\n",
    "\n",
    "* getting data from a URL\n",
    "* [Representational State Transfer (REST)](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm)\n",
    "* Google has a [Dataset Search](https://toolbox.google.com/datasetsearch)\n",
    "\n",
    "My implementation in both Python and R is on Github [here](https://github.com/frankhjung/jupyter-datapipelines).\n",
    "There is also a [Gitlab version](https://gitlab.com/theMarloGroup/jupyter-notebooks/datapipelines) that uses pipelines to execute the scripts.\n",
    "\n",
    "### Creating a Dataset on Kaggle\n",
    "\n",
    "Manufacturing employment as a proportion of total employment is a CSV file:\n",
    "\n",
    "* [2018-Australian SDG-indicator-9-2-2.csv](https://data.gov.au/dataset/7f90d314-fa64-4bae-8609-2e26ff48f6fa/resource/05390642-bdeb-4174-9bd5-c1df6e6d1e9e/download/2018-australian-sdg-indicator-9-2-2v2.csv)\n",
    "\n",
    "Added dataset to Kaggle here: https://www.kaggle.com/frankhjung/australian-manufacturing-employment-2018\n",
    "\n",
    "![Kaggle Dataset](https://raw.githubusercontent.com/frankhjung/jupyter-datapipelines/master/images/dataset-validation.png)\n",
    "\n",
    "### Validating Dataset\n",
    "\n",
    "Before we use the dataset we should ensure that it is valid. For instance we could check:\n",
    "\n",
    "* that file is a valid type (expect CSV)\n",
    "* no missing data (no NA's in data)\n",
    "\n",
    "To perform the validation, it is recommended that a script be run before the data is processed, rather than performing the validation in a notebook.\n",
    "\n",
    "The script is [validation.py](/validation.py).  \n",
    "\n",
    "**HINT** You could save scripts on GitHub, then import as a GitHub data file into Kaggle.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "* [Crisis to Calm: Story of Data Validation @ Netflix](https://www.infoq.com/presentations/data-validation-netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3 - ETL & Creating Datasets from Kernel Output\n",
    "\n",
    "* basic principles of Extract, Transform & Load (aka ETL) pipelines\n",
    "* creating datasets from Kaggle Kernel outputs\n",
    "\n",
    "The notebook for this section is [here](https://www.kaggle.com/rtatman/automating-data-pipelines-day-3).\n",
    "\n",
    "### ETL\n",
    "\n",
    "* **Extracting**: get the raw data you need from where it is being stored\n",
    "* **Transforming**: rearrange that data to fit your needs\n",
    "* **Loading**: storing your transformed data in a different place so it can be used\n",
    "\n",
    "Useful packages:\n",
    "\n",
    "* [rvest](https://cran.r-project.org/web/packages/rvest/index.html) - [easy web scraping](https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/)\n",
    "* [tidytext](https://cran.r-project.org/web/packages/tidytext/index.html) - text mining for word processing and sentiment analysis\n",
    "* [mice](https://cran.r-project.org/web/packages/mice/index.html) - [Mice: Multivariate Imputation By Chained Equations](https://www.rdocumentation.org/packages/mice/versions/3.3.0/topics/mice) for imputation of missing values, see also [MCAR](https://rdrr.io/cran/mice/man/ampute.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kaggle-data-pipelines.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
